This repository contains code for distilling knowledge from a teacher model to a student model using various initial layer selection strategies. 
The project focuses on evaluating the impact of different layer selection methods on the performance of the distilled student model.

The result shows that Distilling with only the even layers resulted in the lowest training loss of 0.2325 and a validation loss of 0.8319. 
Surprisingly, this strategy achieved the highest validation accuracy of 0.6760 among all initial layer selection strategies.
